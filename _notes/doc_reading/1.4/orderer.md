- [ ] 系统链，更新 channel 配置的交易发到哪条链，流程如何
- [ ] 收到交易的顺序不同，orderer 节点间如何协调？问题存在否？？

In addition to their **ordering** role, orderers also maintain the list of organizations that are allowed to **create channels**. This list of organizations is known as the “consortium”, and the list itself is kept in the **configuration** of the “*orderer system channel*”.  
By default, this list, and the channel it lives on, can only be edited by the **orderer admin**. Note that it is possible for an ordering service to hold several of these lists, which makes the consortium a vehicle for Fabric multi-tenancy.

Orderers also enforce basic **access control** for channels, restricting who can read and write data to them, and who can configure them. Who is authorized to modify a configuration element in a channel is subject to the policies that the relevant administrators set when they created the consortium or the channel.  
***Configuration transactions* are processed by the orderer**, as it needs to know the current set of policies to execute its basic form of access control. In this case, the orderer processes the configuration update to make sure that the requestor has the proper administrative rights. If so, the orderer validates the update request against the existing configuration, generates a new configuration transaction, and packages it into a block that is relayed to all peers on the channel. The peers then processes the configuration transactions in order to verify that the modifications approved by the orderer do indeed satisfy the policies defined in the channel.

Just like peers, ordering nodes belong to an organization. And similar to peers, a separate Certificate Authority (CA) should be used for each organization. Whether this CA will function as the root CA, or whether you choose to deploy a root CA and then intermediate CAs associated with that root CA, is up to you.

Phase two: Ordering and packaging transactions into blocks  
Ordering service nodes receive transactions from many different application clients **concurrently**. These ordering service nodes work together to collectively form the ordering service. Its job is to arrange batches of submitted transactions into a well-defined sequence and package them into blocks.  
The number of transactions in a block depends on channel configuration parameters related to the desired size and maximum elapsed duration for a block (`BatchSize` and `BatchTimeout` parameters, to be exact). The blocks are then saved to the **orderer’s ledger** and distributed to all peers that have joined the channel. If a peer happens to be down at this time, or joins the channel later, it will receive the blocks after reconnecting to an ordering service node, or by gossiping with another peer.

The sequencing of transactions in a block is not necessarily the same as the order received by the ordering service, since there can be multiple ordering service nodes that receive transactions at approximately the same time. What’s important is that the ordering service puts the transactions into a strict order, and peers will use this order when validating and committing transactions.  
This strict ordering of transactions within blocks makes Hyperledger Fabric a little different from other blockchains where the same transaction can be packaged into multiple different blocks that **compete** to form a chain. In Hyperledger Fabric, the blocks generated by the ordering service are **final**. Once a transaction has been written to a block, its position in the ledger is immutably assured. Hyperledger Fabric’s finality means that there are no ledger forks — validated transactions will never be reverted or dropped.

Whereas peers execute smart contracts and process transactions, orderers most definitely do not. Every authorized transaction that arrives at an orderer is mechanically packaged in a block — the orderer makes no judgement as to the content of a transaction (except for channel configuration transactions).


Phase three: Validation and commit  
Phase 3 begins with the orderer distributing blocks to all peers connected to it. Not every peer needs to be connected to an orderer — peers can cascade blocks to other peers using the gossip protocol.

Each peer will validate distributed blocks **independently**, but in a **deterministic** fashion, ensuring that ledgers remain **consistent**. Specifically, each peer in the channel will validate each transaction in the block to ensure it has been endorsed by the required organization’s peers, that its endorsements match, and that it hasn’t become invalidated by other recently committed transactions which may have been in-flight when the transaction was originally endorsed. Invalidated transactions are still retained in the immutable block created by the orderer, but they are marked as invalid by the peer and do not update the ledger’s state.


Ordering service implementations  
Solo  
It features only **a single ordering node**. A Solo ordering service is not crash-fault tolerant. For that reason, Solo implementations cannot be considered for production, but they are a good choice for **testing applications and smart contracts**, or for **creating proofs of concept**. However, if you ever want to extend this PoC network into production, you might want to start with a single node Raft cluster, as it may be reconfigured to add additional nodes.  
Solo ordering service processes transactions identically to the more elaborate Kafka and Raft implementations while saving on the administrative overhead of maintaining and upgrading multiple nodes and clusters.

Raft  
New as of v1.4.1, Raft is a *crash fault tolerant* (CFT) ordering service based on an implementation of Raft protocol in etcd. Raft follows a “leader and follower” model, where a leader node is elected (per channel) and that **leader's decisions are replicated by the followers**. Raft ordering services should be **easier** to set up and manage than Kafka-based ordering services, and their design **allows different organizations to contribute nodes to a distributed ordering service**.

Kafka  
Apache Kafka is a CFT implementation that uses a “leader and follower” node configuration. Kafka utilizes a ZooKeeper ensemble for **management purposes**. The Kafka based ordering service has been available since Fabric v1.0, but many users may find the additional administrative overhead of managing a Kafka cluster intimidating or undesirable.

From the perspective of the service they provide to a network or a channel, Raft and the existing Kafka-based ordering service are similar. They’re both CFT ordering services using the leader and follower design. If you are an application developer, smart contract developer, or peer administrator, you will not notice a functional difference between an ordering service based on Raft versus Kafka.  
However, there are a few major differences worth considering, especially if you intend to manage an ordering service:  
1. Raft is easier to set up. Deploying a Kafka cluster and its ZooKeeper ensemble can be tricky. There are many more components to manage with Kafka than with Raft. Kafka has its own versions, which must be coordinated with your orderers. With Raft, everything is **embedded** into your ordering node.  
2. Kafka and Zookeeper are not designed to be run across large networks. They are designed to be CFT but should be run in a tight group of hosts. This means that practically speaking you need to have one organization run the Kafka cluster. Given that, having ordering nodes run by different organizations when using Kafka (which Fabric supports) doesn’t give you much in terms of decentralization because the nodes will all go to the same Kafka cluster which is under the control of a single organization. With Raft, each organization can have its own ordering nodes, participating in the ordering service, which leads to a **more decentralized** system.  
3. Raft is supported natively. While Kafka-based ordering services are currently compatible with Fabric, users are required to get the requisite images and learn how to use Kafka and ZooKeeper on their own. Likewise, support for Kafka-related issues is handled through Apache, the open-source developer of Kafka, not Hyperledger Fabric. The Fabric Raft implementation, on the other hand, has been developed and will be supported within the Fabric developer community and its support apparatus.
4. Where Kafka uses a pool of servers (called “*Kafka brokers*”) and the admin of the orderer organization specifies **how many** nodes they want to use on a particular channel, Raft allows the users to specify **which** ordering nodes will be deployed to which channel. In this way, peer organizations can make sure that, if they also **own an orderer**, this node will be made a part of a ordering service of that channel, rather than trusting and depending on a central admin to manage the Kafka nodes.
5. Raft is the first step toward Fabric’s development of a byzantine fault tolerant (BFT) ordering service. As we’ll see, some decisions in the development of Raft were driven by this. If you are interested in BFT, learning how to use Raft should ease the transition.

Similar to Solo and Kafka, a Raft ordering service can lose transactions after acknowledgement of receipt has been sent to a client. For example, if the leader crashes at approximately the same time as a follower provides acknowledgement of receipt. Therefore, application clients should listen on peers for transaction commit events regardless (to check for transaction validity), but extra care should be taken to ensure that the client also gracefully tolerates a timeout in which the transaction does not get committed in a configured timeframe. Depending on the application, it may be desirable to resubmit the transaction or collect a new set of endorsements upon such a timeout.

Setting up an ordering node
1. Creating the organization your ordering node belongs to
2. Configuring your node (using `orderer.yaml`)
3. Creating the genesis block for the orderer system channel
4. Bootstrapping the orderer

Like peers, all orderers must belong to an organization that must be created before the orderer itself is created. This organization has a definition encapsulated by a Membership Service Provider (MSP) that is created by a Certificate Authority (CA) dedicated to creating the certificates and MSP for the organization.

The configuration of the orderer is handled through `orderer.yaml`. The `FABRIC_CFG_PATH` environment variable is used to point to an `orderer.yaml` file you’ve configured, which will extract a series of files and certificates on your file system.  

A [sample `orderer.yaml`](https://github.com/hyperledger/fabric/blob/release-1.4/sampleconfig/orderer.yaml)

If you are deploying this node as part of a cluster (for example, as part of a cluster of Raft nodes), make note of the `Cluster` and `Consensus` sections.  
If you plan to deploy a Kafka based ordering service, you will need to complete the `Kafka` section.

`LocalMSPID` — this is the name of the MSP, generated by your CA, of your orderer organization. This is where your orderer organization admins will be listed.  
`LocalMSPDir` — the place in your file system where the local MSP is located.  
(`# TLS enabled`) `Enabled: false`- this is where you specify whether you want to enable TLS. If you set this value to true, you will have to specify the locations of the relevant TLS certificates. Note that this is **mandatory for Raft nodes**.
`GenesisFile` — this is the name of the genesis block you will generate for this ordering service.  
`GenesisMethod` — the method by which the genesis block is created. This can be either `file`, in which the file in the `GenesisFile` is specified, and `provisional`, in which the profile in `GenesisProfile` is used.


The first block of a newly created channel is known as a “*genesis block*”. If this genesis block is being created as part of the creation of a new network (in other words, if the orderer being created will not be joined to an existing cluster of orderers), then this genesis block will be the first block of the “*orderer system channel*”, a special channel **managed by the orderer admins** which includes a list of the organizations permitted to create channels. The genesis block of the orderer system channel is special: **it must be created and included in the configuration of the node before the node can be started**.

Start orderer: `docker-compose -f docker-compose-cli.yaml up -d --no-deps orderer.example.com`